services:
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    volumes:
      - ./open-webui/data:/app/backend/data
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ./ollama_data:/root/.ollama
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility

  anythingllm:
    image: mintplexlabs/anythingllm:latest
    container_name: anythingllm
    restart: unless-stopped
    ports:
      - "3001:3001"
    volumes:
      - ./anythingllm_storage:/app/server/storage
    environment:
      # --- Local LLM (Ollama) defaults ---
      - LLM_PROVIDER=ollama
      - OLLAMA_BASE_PATH=http://ollama:11434
      - OLLAMA_MODEL_PREF=llama3.1:8b-instruct-q4_K_M
      - EMBEDDING_ENGINE=ollama
      - STORAGE_DIR=/app/server/storage
      # Optional: if you want explicit embedding model config
      - EMBEDDING_BASE_PATH=http://ollama:11434
      - EMBEDDING_MODEL_PREF=nomic-embed-text:latest

      # --- Cloud LLM (OpenAI) support ---
      # This makes your OpenAI key available inside the container.
      # Youâ€™ll select & configure OpenAI as a provider in the UI.
      - OPEN_AI_KEY=${OPENAI_API_KEY}
      - VECTOR_DB=lancedb
      # Optional: UID/GID if you care about host file permissions
      # - UID=1000
      # - GID=1000
    depends_on:
      - ollama

  # Cloudflare Tunnel to publicly access local llm
  tunnel:    image: cloudflare/cloudflared:latest
    container_name: tunnel
    restart: unless-stopped
    depends_on:
      - anythingllm
    environment:
      # You will paste this from Cloudflare after creating the tunnel
      - TUNNEL_TOKEN=${TUNNEL_TOKEN}
    command: tunnel run
